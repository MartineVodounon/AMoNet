% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RunBackSimul_200519.R
\name{RunBackSimul}
\alias{RunBackSimul}
\title{Runs the back simulation or back propagation of the gradients}
\usage{
RunBackSimul(NETall = NETall, y = y, MUT = NULL, treatmt = NULL,
  Init = NULL, iStates = NULL, Ct = NULL,
  MiniBatch = Default$MiniBatch, Optimizer = Default$Optimizer,
  beta1 = Default$beta1, beta2 = Default$beta2,
  iteration = Default$iteration, learning_rate = Default$learningrate,
  adaptive_iStates = Default$adaptive_iStates, FixNodes = NULL,
  Parallelize = Default$Parallelize, no_cores = Default$no_cores,
  Logic = Default$Logic, Mode = Default$Mode,
  ModeBack = Default$Mode, MinStepsForward = Default$MinStepsForward,
  MinStepsBackward = Default$MinStepsBackward, LSTM = Default$LSTM,
  gradClipping = Default$gradClipping,
  LearningRateDecay = Default$LearningRateDecay,
  ValMut = Default$ValMut, PDF = F, GIF = F, NameProj = "AMoNet",
  Visualize = Default$Visualize, alpha = Default$alpha,
  lambda = Default$lambda)
}
\arguments{
\item{NETall}{data frame. The network in format (similar to .sif with) columns "source", "interaction", "target", additionally storing the architecture (column "Layer"), the parameters and optionnaly Boolean logic.}

\item{y}{numeric or matrix. labels}

\item{MUT}{list. Corresponds to mutations with names and corresponding values (list of vectors). If MUT=NULL, a wild type *AMoNet* is simulated.. Patients ordered the same as y.}

\item{treatmt}{list. The same for than newMUT, with treatments' targets and corresponding values (list of vectors).}

\item{Init}{matrix. Used to set some of the initial states (iStates). To force some initial states during learning set adaptive_iStates=F. Init=NULL otherwise.}

\item{iStates}{matrix. Initial states of the whole network model. Either set from data or randomized.}

\item{MiniBatch}{integer. nb of sample per MiniBatch. For batch learning, use MiniBatch = total nb of examples}

\item{Optimizer}{character. Options are "Adam","Momentum","RMSprop" or NULL. NULL corresponds to gradient descent and can be used with batch learning. Use of optimizers are recommended to mini batch learning.}

\item{beta1}{numeric. Momentum term. Adam is defined by Momentum / RMSprop.}

\item{beta2}{numeric. RMSprop term. Adam is defined by Momentum / RMSprop.}

\item{iteration}{integer. Number of iterations, corresponding to epochs, i.e. one path through the full data.}

\item{FixNodes}{Character. Freeze parameters of selected nodes.}

\item{Parallelize}{boolean. Should the simulation run in parallel (TRUE by default)? Not recommended for simulations of less than 10 examples.}

\item{no_cores}{numeric. If Parallel=TRUE, set the number of cores to parallelize on. Default is 4. Can detect and set to available no_cores if inferior to user defined no_cores. to}

\item{Logic}{character. The update function used for the simulations. Choices are: "Sigmoid" (default), "Boolean","tanh" or "ReLU".}

\item{Mode}{character. The type of simulation to perform. Choice are "LAYER" (layer based, default), "ASYNC" (assynchronous update), "SYNC" (synchronous update)}

\item{MinStepsForward}{numeric. Number of simulation steps. Default to 5. Ideally set the number of simulation steps to achieve stable states of *AMoNet* activities.}

\item{MinStepsBackward}{numeric. Number of back-simulation steps. Default to 1, as in a standard back-propagation of gradient in neural nets.}

\item{LSTM}{boolean. Should a Long-Short Term Memory (LSTM) unit architecture be used?}

\item{gradClipping}{nunmeric. Maximum values allowed for gradients otherwise clipped. Gradient scaling in dev.}

\item{ValMut}{numeric. Multicplicative factor for the effect of mutations. Default to 50.}

\item{Visualize}{character or integer. Options : \code{c(1,"Gates","Output")} Real time visualization of learning, requires selecting the visualization target: "Gates","Output", or all nodes activities of a patients by selecting an integer (a patient's number) or an ID. Default is no visualization: set to \code{NULL}, .}

\item{alpha}{numeric. Regularization term. Set to 1 for L1N (Lasso) regularization, set to 2 for L2N (Ridge) regularization, between 1 and 2 with lambda 1 for L1N + L2N (Elastic Net) regularization (penalization parameter is alpha).}

\item{lambda}{numeric. the lambda value for Lasso, Ridge or Elastic Net regularization. Set to 0 corresponds to no regularization. When alpha between 1 and 2 (Elastic Net regularization), lambda should be set to 1 and penalization parameter is alpha.}
}
\value{
a list with the network dataframe NETall, and for each epoch:
the training Cost, the network weights (NETallList) and the simulation activities (NETallActivity)
}
\description{
\code{RunBackSimul} process minibatches and run :
1. simulations
2. compute error on outputs
3. flip the net
4. simulation of the derivate of the functions
5. correct the parameters with gradients
}
\details{
Preferably use the \code{train()} function from AMoNet package to run the training.

Default hyperparameters used for training are stored in \code{Default}.

For character objects in hyperparameters (for eg Optimizer can be "Adam", "RMSprop, "Momentum" or NULL), options are stored in \code{Boundaries}

\code{MUT} and \code{treatmt} parameters can be generated for a matrix (samples * genes) filled with values and \code{NAs}, using the \code{MutMatToList()} functcion.

\code{MinStepsForward} is corresponding to generic argument \code{nsim} in S3 function \code{simulate()}.

Setting argument \code{MinStepsBackward} > 1 can result in vanishing gradient. Nevertheless, using \code{LSTM=TRUE} in this context may help.
}
\examples{
\dontrun{
#see the \\code{?train()} function from AMoNet package
}
}

% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/S3functions.R
\name{train.AMoNet}
\alias{train.AMoNet}
\alias{train}
\title{AMoNet training}
\usage{
train(object, y, MUT=NULL, treatmt=NULL, Init=NULL, iStates=NULL, KeepTraining=T, KeepData=T, Ct=NULL, MiniBatch=Default$MiniBatch, Optimizer=Default$Optimizer, beta1=Default$beta1, beta2=Default$beta2, iteration=Default$iteration, learning_rate=Default$learningrate, adaptive_iStates=Default$adaptive_iStates, FixNodes=NULL, Parallelize=Default$Parallelize, no_cores=Default$no_cores, Logic = Default$Logic, Mode = Default$Mode, ModeBack=Default$Mode, MinStepsForward = Default$MinStepsForward,MinStepsBackward=Default$MinStepsBackward, LSTM=Default$LSTM, gradClipping=Default$gradClipping, LearningRateDecay=Default$LearningRateDecay, ValMut=Default$ValMut, PDF=F, GIF=F, NameProj=net$call$build_call$NameProj, Visualize=Default$Visualize, alpha=Default$alpha, lambda=Default$lambda)
}
\arguments{
\item{object}{*AMoNet* object, S3 class.}

\item{y}{numeric or matrix. labels}

\item{MUT}{list. Corresponds to mutations with names and corresponding values (list of vectors). If MUT=NULL, a wild type *AMoNet* is simulated.. Patients ordered the same as y.}

\item{treatmt}{list. The same for than newMUT, with treatments' targets and corresponding values (list of vectors).}

\item{Init}{matrix. Used to set some of the initial states (iStates). To force some initial states during learning set adaptive_iStates=F. Init=NULL otherwise.}

\item{iStates}{matrix. Initial states of the simulations.}

\item{KeepTraining}{boolean. If \code{TRUE} (default) stores the training in the *AMoNet* object, which is usefull to plot the training step.}

\item{KeepData}{boolean. If \code{TRUE} (default) stores the data in the *AMoNet* object. Usefull to run prediction on same data as training.}

\item{Ct}{matrix. If LSTM=TRUE, Ct is used to set the initial states of the correponding values in LSTM units.}

\item{MiniBatch}{integer. nb of sample per MiniBatch. For batch learning, use MiniBatch = total nb of examples}

\item{Optimizer}{character. Options are "Adam","Momentum","RMSprop" or NULL. NULL corresponds to gradient descent and can be used with batch learning. Use of optimizers are recommended to mini batch learning.}

\item{beta1}{numeric. Momentum term. Adam is defined by Momentum / RMSprop.}

\item{beta2}{numeric. RMSprop term. Adam is defined by Momentum / RMSprop.}

\item{alpha}{numeric. Regularization term. Set to 1 for L1N (Lasso) regularization, set to 2 for L2N (Ridge) regularization, between 1 and 2 with lambda 1 for L1N + L2N (Elastic Net) regularization (penalization parameter is alpha).}

\item{lambda}{numeric. the lambda value for Lasso, Ridge or Elastic Net regularization. Set to 0 corresponds to no regularization. When alpha between 1 and 2 (Elastic Net regularization), lambda should be set to 1 and penalization parameter is alpha.}

\item{iteration}{integer. Number of iterations, corresponding to epochs, i.e. one path through the full data.}

\item{learning_rate}{numeric. Learning rate value.}

\item{adaptive_iStates}{boolean. Do the initial states of the simulations be updated at each epoch? Deafult is TRUE. Set to FALSE, the initial states of simulations will always be the same.}

\item{FixNodes}{character. Freeze parameters of selected nodes.}

\item{Parallelize}{boolean. Should the simulation run in parallel (TRUE by default)? Not recommended for simulations of less than 10 examples.}

\item{no_cores}{numeric. If Parallel=TRUE, set the number of cores to parallelize on. Default is 4. Can detect and set to available no_cores if inferior to user defined no_cores.}

\item{Logic}{character. The update function used for the simulations. Choices are: "Sigmoid" (default), "Boolean","tanh" or "ReLU".}

\item{Mode}{character. The type of simulation to perform. Choice are "LAYER" (layer based, default), "ASYNC" (assynchronous update), "SYNC" (synchronous update)}

\item{ModeBack}{character. The type of back-simulation to perform. Choice are "LAYER" (layer based, default), "ASYNC" (assynchronous update), "SYNC" (synchronous update)}

\item{MinStepsForward}{numeric. Number of simulation steps. Default to 5. Ideally set the number of simulation steps to achieve stable states of *AMoNet* activities.}

\item{MinStepsBackward}{numeric. Number of back-simulation steps. Default to 1, as in a standard back-propagation of gradient in neural nets.}

\item{LSTM}{boolean. Should a Long-Short Term Memory (LSTM) unit architecture be used?}

\item{gradClipping}{nunmeric. Maximum values allowed for gradients otherwise clipped. Gradient scaling in dev.}

\item{LearningRateDecay}{character. Choice are "linear", "exponential" or NULL.}

\item{ValMut}{numeric. Multicplicative factor for the effect of mutations. Default to 50.}

\item{PDF}{boolean. Whether to save a pdf from visualizations of the learning phase.}

\item{GIF}{boolean. Whether to save a gif from visualizations during the learning phase.}

\item{NameProj}{character. Name of the project.}

\item{Visualize}{character or integer. Options : \code{c(1,"Gates","Output")} Real time visualization of learning, requires selecting the visualization target: "Gates","Output", or all nodes activities of a patients by selecting an integer (a patient's number) or an ID. Default is no visualization: set to \code{NULL}, .}
}
\value{
stores in *AMoNet* object a list with the network dataframe NETall, and for each epoch, if \code{KeepTraining=TRUE}:
the training Cost, the network weights (NETallList) and the simulation activities (NETallActivity)
}
\description{
Runs the \code{RunBackSimul} function (see \code{?RunBackSimul} for detailed description)
}
\details{
The \code{train.AMoNet()} function mainly calls \code{RunBackSimul()} function
}
